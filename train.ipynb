{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6a8b8a-943a-49cb-97d2-25666083a108",
   "metadata": {},
   "source": [
    "# LSTM Harmonizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f04f2c-a8ad-48fa-a7a0-8d83cb3dd249",
   "metadata": {},
   "source": [
    "##### Max Brynolf 2023\n",
    "The code in this notebook trains an LSTM-network to find appropriate chords to melody sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0b328-71b2-4421-a6ff-cf468cc7d8ce",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959fa0d-46d8-4cbd-b6ef-9e342d857531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from music21 import note, chord, converter, stream, midi\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edae8f-936b-4000-a57c-53b3b9b740f6",
   "metadata": {},
   "source": [
    "### File Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb331d28-0274-4b4a-b7f8-7dddff483a40",
   "metadata": {},
   "source": [
    "The following functions are related to translating MIDI-files into tensors that are used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54645483-1ee6-4b67-aa40-5d4530a5783e",
   "metadata": {},
   "source": [
    "The following constants define the output interval for a chord. Notes that are above `pitch_max` get shifted down to the highest note within the interval, and notes that are below `pitch_min` get shifted up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d65b9-e24c-4472-bbad-356fac738dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_max = 72\n",
    "pitch_min = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4a313-0f2a-4dfc-ad44-367d3b25ca54",
   "metadata": {},
   "source": [
    "The following function takes an input chord and returns a new chord within the interval defined by `pitch_min` and `pitch_max`. Moreover, duplicate notes are removed and a custom filter can be applied by letting the function return `[]` when a chord should be excluded from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78df6f-d7ed-4e0a-934d-5b1f3ef721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chord_filter(c):\n",
    "    c = c.removeRedundantPitches()\n",
    "    for n in c.notes:\n",
    "        p = n.pitch.midi\n",
    "        note_name = p % 12\n",
    "        if p < pitch_min:\n",
    "            n.pitch.midi = pitch_min + note_name\n",
    "        elif p > pitch_max:\n",
    "            n.pitch.midi = pitch_max - 12 + note_name\n",
    "    c = c.removeRedundantPitches()\n",
    "    if len(c.notes) > 8 or len(c.notes) == 1:\n",
    "        return []\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97749f9-5227-472b-b55d-a7504df4ea59",
   "metadata": {},
   "source": [
    "The function `part_to_chord_vectors` takes all the notes in the part `chord_part` and groups them into chords within windows of `chord_length` quarter notes, starting at offset `start_offset`. In other words, for a set chord length $\\Delta$ and start offset $t_0$, the notes are categorized into the following intervals, each corresponding to a chord:\n",
    "\n",
    "$$\n",
    "[t_0, t_0 + \\Delta], [t_0 + \\Delta, t_0 + 2\\Delta], \\dots , [t_0 + (n - 1)\\Delta, t_0 + n\\Delta]\n",
    "$$\n",
    "\n",
    "The function returns two lists, representing the extracted chords. The first contains all the temporal offsets of the chords, and the second contains the chords themselves, translated into vectors. The vectors contain $P_{max} - P_{min} + 1$ components each (where $P_{max}$ is `pitch_max` and $P_{min}$ is `pitch_min`), of which each corresponds to a specific note within the interval. If a component is set to $1$, the note is a part of the chord, whereas a component set to $0$ means that the note is not a part of the chord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3cade-b720-4b9c-8149-1871e2ff676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_to_chord_vectors(chord_part, start_offset, chord_length):\n",
    "    chord_offsets = []\n",
    "    chords = []\n",
    "    current_offset = start_offset\n",
    "    while current_offset + chord_length <= chord_part.stream().highestTime:\n",
    "        chord_notes = chord_part.getElementsByOffset(current_offset,\n",
    "                                                     current_offset + chord_length,\n",
    "                                                     includeEndBoundary = False)\n",
    "        current_offset += chord_length\n",
    "        if not chord_notes:\n",
    "            continue # no notes found in the specified interval\n",
    "        c = chord.Chord(list(chord_notes))\n",
    "        c = chord_filter(c)\n",
    "        if not c:\n",
    "            continue # chord filter excludes the chord\n",
    "        chord_offsets.append(chord_notes[0].offset)\n",
    "        chord_vector = [0] * (pitch_max - pitch_min + 1)\n",
    "        vector_indices = [n.pitch.midi - pitch_min for n in c.notes]\n",
    "        for ind in vector_indices:\n",
    "            chord_vector[ind] = 1\n",
    "        chords.append(chord_vector)\n",
    "    return chord_offsets, chords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529944fc-fe16-4832-9cef-2e4323b4c17d",
   "metadata": {},
   "source": [
    "The function `create_training_data` creates the training data. The LSTM-model should take a sequence of melody notes and chords (henceforth referred to as melody-chord-vectors) and predict the next chord, for the upcoming note. Hence, for melody notes $v_i$ and corresponding chords $w_i$, a set of melody-chord-vectors $u_i = (w_i, v_i)$ can be constructed. This is done with respect to the chords extracted from `part_to_chord_vectors`, meaning that all melody notes within a specific chord window gets associated with the corresponding chord. The task of the model $f$ is, for a given sequence of melody-chord-vectors, to predict the next chord $w_{i+1}$, hence:\n",
    "\n",
    "$$\n",
    "f(u_i, u_{i - 1}, u_{i - 2}, \\dots , u_{i - k}) = w_{i + 1}\n",
    "$$\n",
    "\n",
    "Therefore, the training data consists of sequences of melody-chord-vectors and corresponding output chords. The function `create_training_data` translates a melody part `melody_part` into a list of melody-chord-sequences, each of length `sequence_length`, where each sequence leads up to the offsets in `chord_offsets`, i.e. each sequence leads up to the chords. Both the melody-chord sequences, corresponding to the $X$-data, and the chords, corresponding to the $y$-data, are returned. The function also makes sure to delete chords if no new melody notes appear after the previous chord, before returning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40fb70-be75-4f02-90a6-29ff3a5b8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(melody_part, chord_offsets, chords, chord_length, sequence_length):\n",
    "    \n",
    "    # Create melody stream and initialize variables\n",
    "    melody_stream = melody_part.stream()\n",
    "    sequences = [] # melody-chord-sequences\n",
    "    removed_chord_indices = [] # a list pointing to chords that will be removed from the training data\n",
    "    \n",
    "    # Loop through the chords\n",
    "    for i in range(len(chord_offsets)):\n",
    "        \n",
    "        # Initialize a new sequence of sequence_length melody-chord-vectors\n",
    "        sequence = []\n",
    "        previous_offset = chord_offsets[i] # keeps track of the previous offset in the loop below\n",
    "        two_succ_chords = False # whether there has been two successive chords without new melody notes\n",
    "        \n",
    "        # Starting at the chord, go backwards until sequence_length melody-chord-vectors have been added\n",
    "        for j in range(sequence_length):\n",
    "            \n",
    "            # Find the first melody note preceding the current offset\n",
    "            current_offset = melody_stream.getElementBeforeOffset(previous_offset).offset\n",
    "            \n",
    "            # Make sure that there are not two chords in a row\n",
    "            if j == 0 and i != 0 and current_offset < chord_offsets[i - 1]:\n",
    "                two_succ_chords = True\n",
    "                removed_chord_indices.append(i) # if there are, remember to remove the chord later\n",
    "                break\n",
    "            \n",
    "            # Extract the melody note, and in the case of a chord, choose the top note\n",
    "            melody_notes = list(melody_stream.getElementsByOffset(current_offset))\n",
    "            if len(melody_notes) > 1 or type(melody_notes[0]) == chord.Chord:\n",
    "                ch = chord.Chord(melody_notes)\n",
    "                n = ch.sortFrequencyAscending()[-1]\n",
    "                n.offset = current_offset\n",
    "            else:\n",
    "                n = melody_notes[0]\n",
    "            melody_vector = [0] * 88\n",
    "            melody_vector[n.pitch.midi - 21] = 1\n",
    "            \n",
    "            # Find the chord corresponding to the current melody note\n",
    "            k = i\n",
    "            while k >= 0:\n",
    "                if chord_offsets[k] <= n.offset: # chord precedes melody note\n",
    "                    if n.offset - chord_offsets[k] < chord_length: # melody note is played within chord_length\n",
    "                        chord_vector = chords[k]\n",
    "                        break\n",
    "                    else:\n",
    "                        chord_vector = [0] * (pitch_max - pitch_min + 1) # no associated chord\n",
    "                        break\n",
    "                elif k == 0:\n",
    "                    chord_vector = [0] * (pitch_max - pitch_min + 1) # no associated chord\n",
    "                k -= 1\n",
    "            \n",
    "            # Add the total, concatenated vector to the sequence\n",
    "            sequence.append(chord_vector + melody_vector)\n",
    "            previous_offset = current_offset\n",
    "        \n",
    "        # Go to the next chord if there are no new melody notes preceding the chord\n",
    "        if two_succ_chords:\n",
    "            continue\n",
    "        \n",
    "        # Reverse the melody-chord-sequence and add it to the total sequences list\n",
    "        sequence = sequence[::-1]\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    # Remove all chords where there are no new preceding melody notes\n",
    "    for i in removed_chord_indices[::-1]:\n",
    "        del chords[i]\n",
    "    \n",
    "    # Return the melody-chord-sequences and corresponding chords\n",
    "    return sequences, chords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d24b4-4412-4691-9fd6-d31c827df016",
   "metadata": {},
   "source": [
    "The function `midi_to_tensors` converts a MIDI-file at path `path` to tensors that can be used to train the LSTM-network. These tensors contain melody-chord sequences of size `sequence_length`, packed into `X`, and corresponding chords following the given melody-chords, packed into `y`.\n",
    "\n",
    "The parameters `mel_chs` and `acc_chs` specify the channels corresponding to the melody- and accompaniment respectively. For example, if the MIDI-file has one melody channel $0$ and two accompaniment channels $1$ and $2$, then `mel_chs = [0]` and `acc_chs = [1, 2]`. The channels are then merged into a single stream, from which the tensors are extracted.\n",
    "\n",
    "The parameter `chord_length` is passed into `part_to_chord_vectors` and hence decides how long each window is when grouping notes into chords. The parameter `chord_start_offset` sets the offset of the first chord that's on an even beat, so that the grouping of chords is done on correct beats. However, `chord_start_offset` doesn't have to be preceded by `sequence_length` notes, since this is calculated automatically — it is only there to specify where the even beats are. This saves some time when manually processing MIDI-files. Setting `chord_start_offset` to $-1$ makes it the offset of the first chord in the accompaniment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ae8cd-149c-4dd2-a13a-7eb29625eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_tensors(path, mel_chs, acc_chs, chord_length, sequence_length, chord_start_offset):\n",
    "    \n",
    "    # Parse MIDI-file\n",
    "    score = converter.parse(path)\n",
    "    melody = score.parts[mel_chs[0]]\n",
    "    for i in mel_chs[1:]:\n",
    "        for n in score.parts[i].flatten().notes:\n",
    "            melody.insert(n.offset, n)\n",
    "    melody = melody.flatten().notes\n",
    "    accompaniment = score.parts[acc_chs[0]]\n",
    "    for i in acc_chs[1:]:\n",
    "        for n in score.parts[i].flatten().notes:\n",
    "            accompaniment.insert(n.offset, n)\n",
    "    accompaniment = accompaniment.flatten().notes\n",
    "    if chord_start_offset == -1:\n",
    "        chord_start_offset = accompaniment[0].offset # let the first chord be the first beat\n",
    "    \n",
    "    # Extract chords\n",
    "    o = -1\n",
    "    counter = 0\n",
    "    for lower_bound_offset in [float(n.offset) for n in list(melody)]:\n",
    "        if lower_bound_offset != o:\n",
    "            counter += 1\n",
    "        if counter > sequence_length:\n",
    "            break\n",
    "        o = lower_bound_offset\n",
    "    start_offset = chord_start_offset + math.ceil((lower_bound_offset - chord_start_offset)/chord_length) * chord_length\n",
    "    chord_offsets, chords = part_to_chord_vectors(accompaniment, start_offset, chord_length)\n",
    "    \n",
    "    # Extract chord-melody-sequences\n",
    "    chord_mel_sequences, chords = create_training_data(melody,\n",
    "                                                       chord_offsets,\n",
    "                                                       chords,\n",
    "                                                       chord_length,\n",
    "                                                       sequence_length)\n",
    "    \n",
    "    # Transform into tensors\n",
    "    X = torch.FloatTensor(chord_mel_sequences)\n",
    "    y = torch.FloatTensor(chords)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982b976-dc45-4f41-ad30-01c7af9f94ce",
   "metadata": {},
   "source": [
    "### Playback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d2dd9-46f1-4cb8-9949-ba894d5218c2",
   "metadata": {},
   "source": [
    "The following functions are concerned with generating chords when a model is given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac360de6-5033-49ba-b484-e57f92502830",
   "metadata": {},
   "source": [
    "The function `melody_from_midi` extracts melody notes from a MIDI-file and converts them into melody vectors. The channel used is specified as `melody_channel`. Aside from a list with melody vectors, a corresponding offset list is returned, specifying the temporal offsets and lengths of the melody notes. This information is important if the playback should respect the original rhythms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa8630-ae40-4def-87d8-e167c1a9fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melody_from_midi(path, melody_channel):\n",
    "    score = converter.parse(path)\n",
    "    melody = score.parts[melody_channel].flatten().notes\n",
    "    current_note = list(melody)[0]\n",
    "    melody_vectors = []\n",
    "    offset_list  = []\n",
    "    while current_note is not None:\n",
    "    \n",
    "        # Extract the melody note, and in the case of a chord, choose the top note\n",
    "        melody_notes = list(melody.getElementsByOffset(current_note.offset))\n",
    "        if len(melody_notes) > 1 or type(melody_notes[0]) == chord.Chord:\n",
    "            ch = chord.Chord(melody_notes)\n",
    "            n = ch.sortFrequencyAscending()[-1]\n",
    "            n.offset = current_note.offset\n",
    "        else:\n",
    "            n = melody_notes[0]\n",
    "        melody_vector = [0] * 88\n",
    "        melody_vector[n.pitch.midi - 21] = 1\n",
    "        melody_vectors.append(melody_vector)\n",
    "        offset_list.append([current_note.offset, current_note.quarterLength])\n",
    "        \n",
    "        # Proceed to next melody note\n",
    "        next_note = melody.stream().getElementAfterElement(current_note)\n",
    "        while next_note is not None and next_note.offset == current_note.offset:\n",
    "            current_note = next_note\n",
    "            next_note = melody.stream().getElementAfterElement(current_note)\n",
    "        current_note = next_note\n",
    "    \n",
    "    return offset_list, melody_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50404652-8df6-49b0-8754-51ba1ab5472d",
   "metadata": {},
   "source": [
    "The function `generate_chords_from_melody` takes a melody and generates chords to it using the model `net`. It returns a stream that can be written to a MIDI-file. As inputs, it expects an LSTM-network `net`, a list of melody notes `melody_notes`, an offset list `offset_list` with temporal information about the melody notes, and a sequence length `sequence_length` that specifies the length of each sequence that is fed into the LSTM-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769cd52-33a7-4930-b547-d7fa2bc9f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chords_from_melody(net, melody_notes, offset_list, sequence_length):\n",
    "    accompaniment = stream.Part()\n",
    "    accompaniment.insert(music21.instrument.Piano())\n",
    "    melody = stream.Part()\n",
    "    total_offset = 0\n",
    "    def create_note(pitch, length, offset, volume = 127):\n",
    "        n = note.Note()\n",
    "        n.pitch.midi = pitch\n",
    "        n.quarterLength = length\n",
    "        n.offset = offset\n",
    "        n.volume = volume\n",
    "        return n\n",
    "    \n",
    "    # Add chords and melody notes\n",
    "    latest_chords = []\n",
    "    for i in range(sequence_length):\n",
    "        total_offset, note_length = offset_list[i]\n",
    "        current_note = create_note(melody_notes[i].index(1) + 21, note_length, total_offset)\n",
    "        melody.insert(current_note)\n",
    "        latest_chords.append([0] * (pitch_max - pitch_min + 1))\n",
    "    for i in range(sequence_length, len(melody_notes)):\n",
    "        total_offset, note_length = offset_list[i]\n",
    "        current_note = create_note(melody_notes[i].index(1) + 21, note_length, total_offset)\n",
    "        melody.insert(current_note)\n",
    "        input_vector = [list(l) + list(m) for l, m in zip(latest_chords, melody_notes[i - sequence_length : i])]\n",
    "        input = torch.FloatTensor(input_vector).unsqueeze(0)\n",
    "        output = net(input).squeeze().detach().cpu().numpy()\n",
    "        pitches = [i + pitch_min for i, v in enumerate(output) if v > 0.5]\n",
    "        for pitch in pitches:\n",
    "            current_note = create_note(pitch, note_length, total_offset, 80)\n",
    "            accompaniment.insert(current_note)\n",
    "        latest_chords.append(output)\n",
    "        latest_chords.pop(0)\n",
    "    \n",
    "    output_stream = stream.Score()\n",
    "    meta_data = music21.metadata.Metadata()\n",
    "    meta_data.title = \"Harmonized melody\"\n",
    "    output_stream.insert(0, meta_data)\n",
    "    output_stream.insert(0, melody)\n",
    "    output_stream.insert(0, accompaniment)\n",
    "    return output_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7466a36-dcd3-4949-9cbd-ba8ee52ab7e5",
   "metadata": {},
   "source": [
    "### LSTM-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ef51e-1780-4807-8d05-58453e393d24",
   "metadata": {},
   "source": [
    "The class `RNN` defines the architecture of the LSTM-network. It takes a structural argument `layers` upon initialization, defining the network structure. The network is divided into two parts - one that handles the chords and one that handles the melody notes, and each of these branches can contain several LSTM cells.\n",
    "\n",
    "The argument `layers` is a list containing two inner lists along with the output layer size. The first list corresponds to the layers in the melody branch and the second corresponds to the layers in the accompaniment branch. The output layer size is the same as the chord vector size, given by $P_{max} - P_{min} + 1$. For example, if:\n",
    "\n",
    "```\n",
    "layers = [[100, 10], [50, 20, 10], pitch_max - pitch_min + 1]\n",
    "```\n",
    "\n",
    "then the melody notes will be fed to two LSTM cells, the first with $100$ output nodes and the second with $10$ output nodes, and the accompaniment notes will be fed to three LSTM cells with $50$, $20$ and $10$ output nodes respectively. The branches are then fully connected to the output layer, in this case from $10 + 10 = 20$ nodes to $P_{max} - P_{min} + 1$ output nodes.\n",
    "\n",
    "Using this structure, it is possible to balance the influence that the chords and melody has on the output separately. For instance, changing to:\n",
    "\n",
    "```\n",
    "layers = [[100, 10], [50, 20, 5], pitch_max - pitch_min + 1]\n",
    "```\n",
    "\n",
    "gives the accompaniment a smaller influence on the output.\n",
    "\n",
    "Lastly, the methods `train` and `test` allow you to switch between training- and testing mode. In training mode, the raw logits are returned, so that the `BCEWithLogitsLoss` can be used as a loss function. In testing mode, the probabilities are returned instead, i.e. the logits are passed through a sigmoid layer before being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03123559-d8c2-4c9d-a7a5-0022c49a78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.acc_size = layers[2]\n",
    "        self.rnn_mel_layers = nn.ModuleList()\n",
    "        self.rnn_acc_layers = nn.ModuleList()\n",
    "        self.rnn_mel_layers.append(nn.LSTM(88, layers[0][0], 1, batch_first = True))\n",
    "        for i, layer in enumerate(layers[0][0:-1]):\n",
    "            self.rnn_mel_layers.append(nn.LSTM(layer, layers[0][i + 1], 1, batch_first = True))\n",
    "        self.rnn_acc_layers.append(nn.LSTM(layers[2], layers[1][0], 1, batch_first = True))\n",
    "        for i, layer in enumerate(layers[1][0:-1]):\n",
    "            self.rnn_acc_layers.append(nn.LSTM(layer, layers[1][i + 1], 1, batch_first = True))\n",
    "        self.fc = nn.Linear(layers[0][-1] + layers[1][-1], layers[2])\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "        self.training_mode = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_acc = x[:, :, : self.acc_size]\n",
    "        y_mel = x[:, :, self.acc_size :]\n",
    "        for layer in self.rnn_mel_layers:\n",
    "            y_mel, (h, c) = layer(y_mel)\n",
    "        for layer in self.rnn_acc_layers:\n",
    "            y_acc, (h, c) = layer(y_acc)\n",
    "        y = self.fc(torch.cat((y_acc[:, -1, :], y_mel[:, -1, :]), 1))\n",
    "        if not self.training_mode:\n",
    "            y = self.output_activation(y)\n",
    "        return y\n",
    "    \n",
    "    def train(self):\n",
    "        self.training_mode = True\n",
    "    \n",
    "    def test(self):\n",
    "        self.training_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05731c98-f430-4222-aa7e-fd7e23d74f33",
   "metadata": {},
   "source": [
    "### File Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64254d-90ad-4b1c-b45b-8e71cc1e8b79",
   "metadata": {},
   "source": [
    "The function `data_plot` plots information about the data, in such a way that an overview of the dataset can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d515c-69b2-4782-9171-2914115632dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_plot(chord_list, data_families):\n",
    "    def disp_text(x):\n",
    "        return f\"{(x * y.shape[0] / 100):.0f}\\n({x:.0f}%)\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(12)\n",
    "    ax1.pie([t.shape[0] for t in y_families], labels = data_families, autopct=disp_text)\n",
    "    ax1.set_title(\"Data origin distribution\")\n",
    "    chord_sizes = [torch.sum(chord_tensor, dim=1).numpy() for chord_tensor in chord_list]\n",
    "    min_notes = min([size.min() for size in chord_sizes])\n",
    "    max_notes = max([size.max() for size in chord_sizes])\n",
    "    ax2.hist(chord_sizes, np.arange(min_notes - 0.5, max_notes + 1.5, 1), stacked = True)\n",
    "    ax2.set(xlabel = \"Number of notes\")\n",
    "    ax2.set(ylabel = \"Number of chords\")\n",
    "    ax2.legend(labels = data_families)\n",
    "    ax2.set_title(\"Chord size distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ffb2d-5426-4901-8917-05204f9842d9",
   "metadata": {},
   "source": [
    "The following code processes a set of MIDI-files and converts them into tensors using the `midi_to_tensors`-function. The files are assumed to have the following paths:\n",
    "\n",
    "> `data_family`/`file_identifier`_1.mid\n",
    "> \n",
    "> `data_family`/`file_identifier`_2.mid\n",
    ">\n",
    "> ...\n",
    ">\n",
    "> `data_family`/`file_identifier`_`n_files`.mid\n",
    "\n",
    "Note that doing this might take some time, depending on the files. Because of this, the `X` and `y` tensors are saved in \"data/X_`file_identifier`.pt\" and \"data/y_`file_identifier`.pt\" so that they can be accessed in the future without having to process the MIDI-files again.\n",
    "\n",
    "In order to properly read the files, the XML file `training_info.xml` should contain information about the melody channels, accompaniment channels, chord length and chord start offset for each file. For example:\n",
    "\n",
    "```xml\n",
    "<data-families>\n",
    "    <data-family name=\"file_identifier\">\n",
    "        <file number=\"1\">\n",
    "            <mel>0<mel>\n",
    "            <mel>2</mel>\n",
    "            ...\n",
    "            <acc>1</acc>\n",
    "            <acc>3</acc>\n",
    "            ...\n",
    "            <chord-length>1</chord-length>\n",
    "            <start-offset>-1</start-offset>\n",
    "        </file>\n",
    "        <file number=\"2\">\n",
    "            ...\n",
    "        </file>\n",
    "        ...\n",
    "    </data-family>\n",
    "</data-families>\n",
    "```\n",
    "\n",
    "Note that specifying the number attribute for the file elements isn't necessary but recommended for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6439c07-1660-4db8-8d4f-3918ff22d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which data to process\n",
    "data_family = \"\"\n",
    "file_identifier = \"\"\n",
    "sequence_length = 8\n",
    "excluded_files = [] # allows you to exclude files manually\n",
    "\n",
    "# Extract training information about the data from training_info.xml\n",
    "training_info_xml = ElementTree.parse(\"training_info.xml\")\n",
    "data_family_xml = training_info_xml.getroot().findall(f\".//data-family[@name='{data_family}']\")\n",
    "assert len(data_family_xml) == 1, f\"training_info.xml should contain exactly one data-family element with name {data_family}\"\n",
    "training_info = []\n",
    "for file in data_family_xml[0]:\n",
    "    mels = []\n",
    "    accs = []\n",
    "    chord_length = 1\n",
    "    start_offset = -1\n",
    "    for file_data in file:\n",
    "        if file_data.tag == \"mel\":\n",
    "            mels.append(int(file_data.text))\n",
    "        if file_data.tag == \"acc\":\n",
    "            accs.append(int(file_data.text))\n",
    "        if file_data.tag == \"chord-length\":\n",
    "            chord_length = float(file_data.text)\n",
    "        if file_data.tag == \"start-offset\":\n",
    "            start_offset = float(file_data.text)\n",
    "    training_info.append({\"mel\": mels, \"acc\": accs, \"chord-length\": chord_length, \"start-offset\": start_offset})\n",
    "\n",
    "# Process the files\n",
    "n_files = len(training_info)\n",
    "trainloaders = []\n",
    "X_list = []\n",
    "y_list = []\n",
    "print(\"Processing files...\")\n",
    "for i in range(1, n_files + 1):\n",
    "    if i in excluded_files:\n",
    "        continue\n",
    "    filename = f\"MIDIs/{data_family}/{file_identifier}_{i}.mid\"\n",
    "    mel_ch = training_info[i - 1][\"mel\"]\n",
    "    acc_ch = training_info[i - 1][\"acc\"]\n",
    "    c_length = training_info[i - 1][\"chord-length\"]\n",
    "    c_offset = training_info[i - 1][\"start-offset\"]\n",
    "    X, y = midi_to_tensors(filename, mel_ch, acc_ch, c_length, sequence_length, c_offset)\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "    print(f\"File {filename} has been processed.\")\n",
    "torch.save(X_list, f\"data/X_{file_identifier}.pt\")\n",
    "torch.save(y_list, f\"data/y_{file_identifier}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8faae-ba62-43c6-8ef5-c853d672779b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e399b-1b3d-4cf5-9c60-4d9d68824f97",
   "metadata": {},
   "source": [
    "The code below decides which data to include and plots the distribution of chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de6245-a6a9-4922-afd8-caf2f0118b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_families = [] # data families to include\n",
    "exclude_sizes = [] # list of chord sizes to exclude from the data\n",
    "\n",
    "X_families = []\n",
    "y_families = []\n",
    "for data_family in data_families:\n",
    "    X_list = torch.load(f\"data/X_{data_family}.pt\")\n",
    "    y_list = torch.load(f\"data/y_{data_family}.pt\")\n",
    "    X_families.append(torch.cat(X_list))\n",
    "    y_families.append(torch.cat(y_list))\n",
    "for i in range(len(y_families)):\n",
    "    num_of_notes = torch.sum(y_families[i], dim = 1)\n",
    "    if len(exclude_sizes) > 0:\n",
    "        condition = num_of_notes != exclude_sizes[0]\n",
    "        for j in exclude_sizes[1:]:\n",
    "            condition = torch.logical_and(condition, num_of_notes != j)\n",
    "        y_families[i] = y_families[i][condition]\n",
    "        X_families[i] = X_families[i][condition]\n",
    "X = torch.cat(X_families)\n",
    "y = torch.cat(y_families)\n",
    "\n",
    "data_plot(y_families, data_families)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ddc8c6-4951-4794-9d0c-f10035c52b80",
   "metadata": {},
   "source": [
    "The following code trains the LSTM network with the data from `X` and `y`. A number of hyperparameters can be tuned: the number of epochs, the LSTM structure, the batch size and the learning rate. Moreover, the split ratio `split_ratio` defines the percentage of the train-test-split. The lists `train_losses` and `test_losses` are appended with train- and test loss data respectively, as the model is being trained.\n",
    "\n",
    "Each output chord is represented by a vector with $P_{max} - P_{min} + 1$ components, where each component corresponds to a note. For example:\n",
    "\n",
    "$$\n",
    "y_i = [0, 0, 0, \\dots, 0, 1, 0, 0, 1, 0, 1, \\dots 0, 0, 0]\n",
    "$$\n",
    "\n",
    "where $y$ is a batch of several chords. Each melody note is also represented by a vector, with the difference that only one component can be $1$ for each melody vector. The melody vectors have $88$ components and cover the entire piano, i.e. from A0 to C8. The inputs to the model consist of melody-chord-vectors, with chords constituting the first $P_{max} - P_{min} + 1$ components, and melody notes constituting the last $88$ components.\n",
    "\n",
    "$$\n",
    "X_i = [\\underbrace{1, 0, 1, \\dots , 0, 1, 0}_\\text{chord}, \\underbrace{0, 0, 0, 0, 0, \\dots , 0, 1, 0, \\dots , 0, 0, 0, 0}_\\text{melody}]\n",
    "$$\n",
    "\n",
    "The training data consists of sequences of melody-chord vectors $X$ along with corresponding chords $y$. The optimization used is RMSprop with binary cross entropy loss. For a batch of $n$ chords, each of size $m = 88 + P_{max} - P_{min} + 1$, the loss is defined by the mean of the Binary Cross Entropy loss for each note, hence:\n",
    "\n",
    "$$\n",
    "J(y, \\hat{y}) = -\\frac{1}{mn}\\sum_{i = 1}^{n} \\sum_{j = 1}^m \\left( y_{i,j} \\log \\sigma (\\hat{y}_{i,j}) + \\left( 1 - y_{i,j} \\right) \\log \\left( 1 - \\sigma (\\hat{y}_{i,j}) \\right) \\right)\n",
    "$$\n",
    "\n",
    "where $\\hat{y} = f(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a5cf4-326a-4043-abd2-1cf28bcf6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 500\n",
    "network_layers = [[100], [30], pitch_max - pitch_min + 1]\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "split_ratio = 0.9\n",
    "update_freq = 10 # print loss information every update_freq epoch\n",
    "\n",
    "# Optimizer and loss function\n",
    "device = torch.device(\"mps\")\n",
    "net = RNN(network_layers).to(device)\n",
    "net.train()\n",
    "best_net = RNN(network_layers).to(device)\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr = lr)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = data.TensorDataset(X, y)\n",
    "training_dataset, test_dataset = data.random_split(dataset, [split_ratio, 1 - split_ratio])\n",
    "trainloader = data.DataLoader(training_dataset, shuffle = True, batch_size = batch_size)\n",
    "testloader = data.DataLoader(test_dataset, shuffle = True, batch_size = batch_size)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    for i, data_list in enumerate(trainloader):\n",
    "        X_batch = data_list[0]\n",
    "        y_batch = data_list[1]\n",
    "        optimizer.zero_grad()\n",
    "        output = net(X_batch.to(device))\n",
    "        target = y_batch.to(device)\n",
    "        loss = bce_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= i + 1\n",
    "    for i, data_list in enumerate(testloader):\n",
    "        X_batch = data_list[0]\n",
    "        y_batch = data_list[1]\n",
    "        output = net(X_batch.to(device))\n",
    "        target = y_batch.to(device)\n",
    "        loss = bce_loss(output, target)\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= i + 1\n",
    "    if epoch == 0:\n",
    "        best_test_loss = test_loss\n",
    "    elif test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        best_net_epoch = epoch\n",
    "        best_net.load_state_dict(net.state_dict())\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if epoch % update_freq == 0:\n",
    "        m = train_losses[-1 * update_freq :]\n",
    "        n = test_losses[-1 * update_freq :]\n",
    "        print(f\"Epoch {epoch}, average train loss: {sum(m)/len(m):.4f}, average test loss: {sum(n)/len(n):.4f}\")\n",
    "print(f\"Training finished! Best performance found at epoch {best_net_epoch} with loss {best_test_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff42c0-37c1-49ec-82a9-c707c137f996",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4b149-5c81-449b-9607-96dd2691e8a6",
   "metadata": {},
   "source": [
    "The cell below plots the train- and test losses of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbf146-6b51-4e1e-828e-05bfe2ae1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = \"Training loss\")\n",
    "plt.plot(test_losses, label = \"Test loss\")\n",
    "plt.plot(test_losses, label = \"Best net\", markevery=[best_net_epoch], ls=\"\", marker=\"o\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee5c45-30f7-4027-80d7-47ca82dc84ac",
   "metadata": {},
   "source": [
    "The cell below saves the model. Note that this saved model can be immediately accessed by the Flask application and hence used when testing the model interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383d436-0ec1-4c50-97ed-f143c2e7851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"structure\": network_layers,\n",
    "            \"model\": best_net.cpu().state_dict(),\n",
    "            \"range\": [pitch_min, pitch_max]},\n",
    "            'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbb60f-973b-48db-8413-5061aeda9f9a",
   "metadata": {},
   "source": [
    "The cell below loads an existing model. Note that this can be used to proceed training after a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88f85d-4bb9-4da0-83a6-caed28c9ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(\"model.pt\")\n",
    "net = RNN(loaded_model[\"structure\"])\n",
    "net.load_state_dict(loaded_model[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b9cb4-4df3-4cef-9de9-419786126a90",
   "metadata": {},
   "source": [
    "The cell below extracts the melody from a specified MIDI-file and applies the model to it. The generated chords along with the corresponding melody notes are written to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc125e-9ad2-4c75-a642-d2eee3dd026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"\"\n",
    "net = net.to(torch.device(\"cpu\"))\n",
    "print(\"Extracting melody...\")\n",
    "o, m = melody_from_midi(file_path, 0)\n",
    "print(\"Generating chords...\")\n",
    "f = generate_chords_from_melody(net, m, o, 8)\n",
    "print(\"Writing to file...\")\n",
    "f.write(\"midi\", f\"harmonized_melody.mid\")\n",
    "print(f\"Finished! File saved as \\\"harmonized_melody.mid\\\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
